{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a418c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/manmath/Desktop/MyProjects/CNN-Text-Classifiers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(\"../\")  \n",
    "print(\"Current working directory:\", Path().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20f0cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manmath/miniconda3/envs/nlp_env/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "696273b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists? True   Absolute path: /home/manmath/Desktop/MyProjects/CNN-Text-Classifiers/Data/FakeNewsNet.csv\n",
      "\n",
      "COLUMNS:\n",
      " ['title', 'news_url', 'source_domain', 'tweet_num', 'real']\n",
      "\n",
      "HEAD (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>news_url</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>tweet_num</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kandi Burruss Explodes Over Rape Accusation on...</td>\n",
       "      <td>http://toofab.com/2017/05/08/real-housewives-a...</td>\n",
       "      <td>toofab.com</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People's Choice Awards 2018: The best red carp...</td>\n",
       "      <td>https://www.today.com/style/see-people-s-choic...</td>\n",
       "      <td>www.today.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sophia Bush Sends Sweet Birthday Message to 'O...</td>\n",
       "      <td>https://www.etonline.com/news/220806_sophia_bu...</td>\n",
       "      <td>www.etonline.com</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Colombian singer Maluma sparks rumours of inap...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-33655...</td>\n",
       "      <td>www.dailymail.co.uk</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gossip Girl 10 Years Later: How Upper East Sid...</td>\n",
       "      <td>https://www.zerchoo.com/entertainment/gossip-g...</td>\n",
       "      <td>www.zerchoo.com</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Kandi Burruss Explodes Over Rape Accusation on...   \n",
       "1  People's Choice Awards 2018: The best red carp...   \n",
       "2  Sophia Bush Sends Sweet Birthday Message to 'O...   \n",
       "3  Colombian singer Maluma sparks rumours of inap...   \n",
       "4  Gossip Girl 10 Years Later: How Upper East Sid...   \n",
       "\n",
       "                                            news_url        source_domain  \\\n",
       "0  http://toofab.com/2017/05/08/real-housewives-a...           toofab.com   \n",
       "1  https://www.today.com/style/see-people-s-choic...        www.today.com   \n",
       "2  https://www.etonline.com/news/220806_sophia_bu...     www.etonline.com   \n",
       "3  https://www.dailymail.co.uk/news/article-33655...  www.dailymail.co.uk   \n",
       "4  https://www.zerchoo.com/entertainment/gossip-g...      www.zerchoo.com   \n",
       "\n",
       "   tweet_num  real  \n",
       "0         42     1  \n",
       "1          0     1  \n",
       "2         63     1  \n",
       "3         20     1  \n",
       "4         38     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dtypes:\n",
      "title            object\n",
      "news_url         object\n",
      "source_domain    object\n",
      "tweet_num         int64\n",
      "real              int64\n",
      "dtype: object\n",
      "\n",
      "Any nulls in columns?\n",
      "title              0\n",
      "news_url         330\n",
      "source_domain    330\n",
      "tweet_num          0\n",
      "real               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC (run this to see exactly what columns your CSV has and a few rows)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "p = Path(\"/home/manmath/Desktop/MyProjects/CNN-Text-Classifiers/Data/FakeNewsNet.csv\")\n",
    "print(\"Exists?\", p.exists(), \"  Absolute path:\", p.resolve())\n",
    "df = pd.read_csv(p)\n",
    "print(\"\\nCOLUMNS:\\n\", df.columns.tolist())\n",
    "print(\"\\nHEAD (first 5 rows):\")\n",
    "display(df.head(5))\n",
    "print(\"\\nDtypes:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nAny nulls in columns?\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd03e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts: {1: 17441, 0: 5755}\n",
      "Final counts → Train: 4800  Test: 1200\n",
      "Sample row: {'text': 'Emmy Snubs: Oprah Winfrey Snub & 5 Others Who Deserved a Nomination – Variety', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(\"/home/manmath/Desktop/MyProjects/CNN-Text-Classifiers/Data/FakeNewsNet.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH.resolve()}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "TEXT_COL = \"title\"       \n",
    "LABEL_COL = \"real\"       \n",
    "\n",
    "df = df[[TEXT_COL, LABEL_COL]].dropna()\n",
    "\n",
    "df = df.rename(columns={TEXT_COL: \"text\", LABEL_COL: \"label\"})\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Value counts:\", df[\"label\"].value_counts().to_dict())\n",
    "\n",
    "SAMPLE_PER_CLASS = 3000\n",
    "\n",
    "dfs = []\n",
    "for cls in sorted(df[\"label\"].unique()):\n",
    "    subset = df[df[\"label\"] == cls]\n",
    "    if len(subset) > SAMPLE_PER_CLASS:\n",
    "        subset = subset.sample(SAMPLE_PER_CLASS, random_state=42)\n",
    "    dfs.append(subset)\n",
    "\n",
    "balanced_df = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    balanced_df,\n",
    "    test_size=0.2,\n",
    "    stratify=balanced_df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Final counts → Train:\", len(train_df), \" Test:\", len(test_df))\n",
    "print(\"Sample row:\", train_df.iloc[0].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b259146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Keys:  ['input_ids', 'attention_mask', 'labels']\n",
      "input_ids shape:  torch.Size([32, 29])\n",
      "attention_mask shape:  torch.Size([32, 29])\n",
      "labels shape:  torch.Size([32])\n",
      "Sample Labels (first 8):  [1, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "TOKENIZER_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "MAX_LEN = 200\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=MAX_LEN):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        txt = str(self.texts[index])\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length= self.max_length,\n",
    "            padding=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        label = int(self.labels[index])\n",
    "        encoded[\"labels\"] = label\n",
    "\n",
    "        return encoded\n",
    "    \n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = NewsDataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), tokenizer, max_length=MAX_LEN)\n",
    "test_ds = NewsDataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist(), tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch Keys: \", list(batch.keys()))\n",
    "print(\"input_ids shape: \", batch[\"input_ids\"].shape)\n",
    "print(\"attention_mask shape: \", batch[\"attention_mask\"].shape)\n",
    "print(\"labels shape: \", batch[\"labels\"].shape)\n",
    "print(\"Sample Labels (first 8): \", batch[\"labels\"][:8].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9466bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size:int,\n",
    "            embed_dim:int = 200,\n",
    "            num_filters:int = 100,\n",
    "            kernel_sizes:tuple = (3, 4, 5),\n",
    "            dropout:float = 0.5,\n",
    "            pad_idx:int = 0,\n",
    "            num_classes:int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embed_dim))\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*num_filters, num_classes)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for conv in self.convs:\n",
    "            nn.init.kaiming_uniform_(conv.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        if self.embedding.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight[self.embedding.padding_idx].zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        emb = self.embedding(input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1).to(emb.dtype)\n",
    "            emb = emb*mask\n",
    "        \n",
    "        emb = emb.unsqueeze(1)\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for conv in self.convs:\n",
    "            x = conv(emb)\n",
    "            x = F.relu(x)\n",
    "            x = x.squeeze(3)\n",
    "\n",
    "            x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "            pooled_outputs.append(x)\n",
    "        \n",
    "        cat = torch.cat(pooled_outputs, dim=1)\n",
    "\n",
    "        dropped = self.dropout(cat)\n",
    "        logits = self.fc(dropped)\n",
    "\n",
    "        if logits.size(1) == 1:\n",
    "            return logits.squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da14de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using tokenizer vocab: 30522 pad_id: 0\n",
      "output shape: torch.Size([8])\n",
      "sample logits (first 5): [0.06497481 0.07232034 0.13559815 0.14432389 0.03916262]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "try:\n",
    "    vocab_size = len(tokenizer.get_vocab())\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    print(\"Using tokenizer vocab:\", vocab_size, \"pad_id:\", pad_id)\n",
    "except Exception:\n",
    "    vocab_size = 5000   \n",
    "    pad_id = 0\n",
    "    print(\"Tokenizer not found in scope — using fallback vocab_size=5000, pad_id=0\")\n",
    "\n",
    "model = TextCNN(vocab_size=vocab_size, embed_dim=200, num_filters=100, kernel_sizes=(3,4,5), pad_idx=pad_id).to(DEVICE)\n",
    "\n",
    "B = 8\n",
    "L = 120\n",
    "batch_input = torch.randint(low=0, high=vocab_size, size=(B, L), dtype=torch.long, device=DEVICE)\n",
    "batch_mask  = (batch_input != pad_id).long().to(DEVICE)\n",
    "\n",
    "out = model(batch_input, batch_mask)  \n",
    "print(\"output shape:\", out.shape)\n",
    "print(\"sample logits (first 5):\", out[:5].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1609b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: False\n",
      "Runtime CUDA: 12.1\n",
      "Device name: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Runtime CUDA:\", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d523be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manmath/miniconda3/envs/nlp_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on device: cpu --- Train Batches 150, Val Batches 38\n",
      "  step 0050  avg_loss=0.6907   acc=0.5337\n",
      "  step 0100  avg_loss=0.6871   acc=0.5837\n",
      "  step 0150  avg_loss=0.6812   acc=0.6185\n",
      "Epoch 01  Train Loss = 0.6812  Train acc = 0.6185  Val Loss = 0.6600  Val Acc = 0.7242  Time = 8.7s\n",
      "  -> Saved Best Model: Models/textcnn_best.pt\n",
      "  step 0050  avg_loss=0.6313   acc=0.7900\n",
      "  step 0100  avg_loss=0.6116   acc=0.7809\n",
      "  step 0150  avg_loss=0.5902   acc=0.7825\n",
      "Epoch 02  Train Loss = 0.5902  Train acc = 0.7825  Val Loss = 0.5489  Val Acc = 0.7483  Time = 8.3s\n",
      "  -> Saved Best Model: Models/textcnn_best.pt\n",
      "  step 0050  avg_loss=0.4655   acc=0.8219\n",
      "  step 0100  avg_loss=0.4562   acc=0.8206\n",
      "  step 0150  avg_loss=0.4487   acc=0.8196\n",
      "Epoch 03  Train Loss = 0.4487  Train acc = 0.8196  Val Loss = 0.5004  Val Acc = 0.7675  Time = 8.4s\n",
      "  -> Saved Best Model: Models/textcnn_best.pt\n",
      "  step 0050  avg_loss=0.3270   acc=0.8956\n",
      "  step 0100  avg_loss=0.3155   acc=0.8962\n",
      "  step 0150  avg_loss=0.3190   acc=0.8892\n",
      "Epoch 04  Train Loss = 0.3190  Train acc = 0.8892  Val Loss = 0.5090  Val Acc = 0.7508  Time = 8.5s\n",
      "  step 0050  avg_loss=0.2312   acc=0.9369\n",
      "  step 0100  avg_loss=0.2247   acc=0.9325\n",
      "  step 0150  avg_loss=0.2241   acc=0.9317\n",
      "Epoch 05  Train Loss = 0.2241  Train acc = 0.9317  Val Loss = 0.5478  Val Acc = 0.7533  Time = 7.9s\n",
      "Early Stopping No Improvement.\n",
      "  step 0050  avg_loss=0.1444   acc=0.9656\n",
      "  step 0100  avg_loss=0.1477   acc=0.9628\n",
      "  step 0150  avg_loss=0.1538   acc=0.9579\n",
      "Epoch 06  Train Loss = 0.1538  Train acc = 0.9579  Val Loss = 0.5642  Val Acc = 0.7550  Time = 7.8s\n",
      "Early Stopping No Improvement.\n",
      "  step 0050  avg_loss=0.1362   acc=0.9656\n",
      "  step 0100  avg_loss=0.1347   acc=0.9619\n",
      "  step 0150  avg_loss=0.1282   acc=0.9627\n",
      "Epoch 07  Train Loss = 0.1282  Train acc = 0.9627  Val Loss = 0.5930  Val Acc = 0.7442  Time = 8.5s\n",
      "Early Stopping No Improvement.\n",
      "  step 0050  avg_loss=0.1018   acc=0.9744\n",
      "  step 0100  avg_loss=0.1067   acc=0.9734\n",
      "  step 0150  avg_loss=0.1055   acc=0.9738\n",
      "Epoch 08  Train Loss = 0.1055  Train acc = 0.9738  Val Loss = 0.6100  Val Acc = 0.7417  Time = 8.6s\n",
      "Early Stopping No Improvement.\n",
      "Total Training Tine:  66.9559485912323\n",
      "Loaded best model for final eval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33220/2779925268.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval: val_loss = 0.5003 val_acc = 0.7675\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "EPOCHS = 8\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch.get(\"attention_mask\", None)\n",
    "        if attn is not None:\n",
    "            attn = attn.to(device)\n",
    "        labels = batch[\"labels\"].float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask=attn)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "        correct += (preds == labels.long()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if step % PRINT_EVERY == 0:\n",
    "            avg_loss = running_loss / total if total else 0.0\n",
    "            acc = correct / total if total else 0.0\n",
    "            print(f\"  step {step:04d}  avg_loss={avg_loss:.4f}   acc={acc:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / total if total else 0.0\n",
    "    epoch_acc = correct /total if total else 0.0\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def eval_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch.get(\"attention_mask\", None)\n",
    "            if attn is not None:\n",
    "                attn = attn.to(device)\n",
    "            labels = batch[\"labels\"].float().to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attn)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_loss = running_loss / total if total else 0.0\n",
    "    val_acc = correct / total if total else 0.0\n",
    "    return val_loss, val_acc\n",
    "\n",
    "model = TextCNN(vocab_size=vocab_size, embed_dim=200, num_filters=100, kernel_sizes=(3, 4, 5), pad_idx=pad_id, dropout=0.5)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "no_improve = 0\n",
    "best_path = \"Models/textcnn_best.pt\"\n",
    "import os\n",
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "\n",
    "print(f\"Train on device: {DEVICE} --- Train Batches {len(train_loader)}, Val Batches {len(test_loader)}\")\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    e0 = time.time()\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
    "    val_loss, val_acc = eval_epoch(model, test_loader, loss_fn, DEVICE)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  Train Loss = {train_loss:.4f}  Train acc = {train_acc:.4f}  Val Loss = {val_loss:.4f}  Val Acc = {val_acc:.4f}  Time = {(time.time()-e0):.1f}s\")\n",
    "\n",
    "    if val_loss + 1e-9 < best_val:\n",
    "        best_val = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  -> Saved Best Model: {best_path}\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"Early Stopping No Improvement.\")\n",
    "\n",
    "print(\"Total Training Tine: \", time.time()-t0)\n",
    "\n",
    "if os.path.exists(best_path):\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    print(\"Loaded best model for final eval.\")\n",
    "val_loss, val_acc = eval_epoch(model, test_loader, loss_fn, DEVICE)\n",
    "print(f\"Final eval: val_loss = {val_loss:.4f} val_acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079bfc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['title', 'news_url', 'source_domain', 'tweet_num', 'real']\n",
      "                                               title  real\n",
      "0  Kandi Burruss Explodes Over Rape Accusation on...     1\n",
      "1  People's Choice Awards 2018: The best red carp...     1\n",
      "2  Sophia Bush Sends Sweet Birthday Message to 'O...     1\n",
      "3  Colombian singer Maluma sparks rumours of inap...     1\n",
      "4  Gossip Girl 10 Years Later: How Upper East Sid...     1\n",
      "5  Gwen Stefani Got Dumped by Blake Shelton Over ...     0\n",
      "6  Broward County Sheriff Fired For Lying About P...     0\n",
      "7  Amber Rose Shuts Down French Montana Dating Ru...     0\n",
      "8  Mindy Kaling makes first post-baby appearance ...     1\n",
      "9  Katharine McPhee Butchers Tony Nominations: “I...     1\n",
      "Value counts for 'real':\n",
      " real\n",
      "1    17441\n",
      "0     5755\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows used for train/test (first 6):\n",
      "                                                   title  real\n",
      "4291   Will Stabler return to 'Law and Order: SVU' fo...     1\n",
      "14922  Travis Barker Survives ‘Really Bad’ Crash with...     1\n",
      "19868  Iggy Azalea's Revenge on Nick Young: I Burned ...     1\n",
      "12669  We have reason to believe a new Justin Timberl...     1\n",
      "12570  23 Times Blake Lively And Ryan Reynolds Trolle...     1\n",
      "3511   Is Chris Hemsworth Done Playing Thor After 'Av...     1\n"
     ]
    }
   ],
   "source": [
    "# run in the same env / notebook where you ran training\n",
    "import pandas as pd\n",
    "p = \"/home/manmath/Desktop/MyProjects/CNN-Text-Classifiers/Data/FakeNewsNet.csv\"   # update if your path differs\n",
    "df = pd.read_csv(p)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df[['title','real']].head(10))\n",
    "print(\"Value counts for 'real':\\n\", df['real'].value_counts())\n",
    "# If you mapped columns to 'text' and 'label' earlier, check sample mapping:\n",
    "print(\"\\nSample rows used for train/test (first 6):\")\n",
    "print(df[['title','real']].sample(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4094bb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded checkpoint OK\n",
      "{'text': 'Breaking news!!! Elon Musk buys the moon!!', 'probability': 0.2508, 'prediction': 'FAKE', 'label': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33220/930945198.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"Models/textcnn_best.pt\")   \n",
    "TOKENIZER_NAME = \"distilbert-base-uncased\"   \n",
    "MAX_LEN = 200\n",
    "EMBED_DIM = 200       \n",
    "NUM_FILTERS = 100      \n",
    "KERNEL_SIZES = (3,4,5)  \n",
    "PAD_IDX_FALLBACK = 0\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "\n",
    "class TextCNNConv2D(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_filters: int, kernel_sizes=(3,4,5), pad_idx=0, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embed_dim))\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for c in self.convs:\n",
    "            nn.init.kaiming_uniform_(c.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        emb = self.embedding(input_ids)          \n",
    "        emb_2d = emb.unsqueeze(1)               \n",
    "\n",
    "        conv_outs = []\n",
    "        for conv in self.convs:\n",
    "            c = conv(emb_2d)                \n",
    "            c = torch.relu(c)\n",
    "            c = c.squeeze(3)                   \n",
    "            c = torch.max(c, dim=2).values    \n",
    "            conv_outs.append(c)\n",
    "\n",
    "        cat = torch.cat(conv_outs, dim=1)     \n",
    "        x = self.dropout(cat)\n",
    "        logits = self.fc(x).squeeze(-1)\n",
    "        return logits\n",
    "    \n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "pad_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "model = TextCNNConv2D(vocab_size=vocab_size, embed_dim=EMBED_DIM,\n",
    "                      num_filters=NUM_FILTERS, kernel_sizes=KERNEL_SIZES, pad_idx=pad_id).to(DEVICE)\n",
    "state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state)   \n",
    "print(\"Loaded checkpoint OK\")\n",
    "\n",
    "def predict(text: str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        prob = torch.sigmoid(logits)\n",
    "\n",
    "        label = 1 if prob.item() >= 0.5 else 0\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"probability\": round(prob.item(), 4),\n",
    "            \"prediction\": \"FAKE\" if label == 0 else \"REAL\",\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "print(predict(\"Breaking news!!! Elon Musk buys the moon!!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681328f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking news!!! Elon Musk buys the moon!! -> {'text': 'Breaking news!!! Elon Musk buys the moon!!', 'probability': 0.2508, 'prediction': 'FAKE', 'label': 0}\n",
      "The president announced new policies today in a press conference. -> {'text': 'The president announced new policies today in a press conference.', 'probability': 0.6969, 'prediction': 'REAL', 'label': 1}\n",
      "Scientists discover cure for rare disease after long research. -> {'text': 'Scientists discover cure for rare disease after long research.', 'probability': 0.6018, 'prediction': 'REAL', 'label': 1}\n",
      "You won't believe these celebrity secrets!! Click to read! -> {'text': \"You won't believe these celebrity secrets!! Click to read!\", 'probability': 0.5369, 'prediction': 'REAL', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    " \"Breaking news!!! Elon Musk buys the moon!!\",\n",
    " \"The president announced new policies today in a press conference.\",\n",
    " \"Scientists discover cure for rare disease after long research.\",\n",
    " \"You won't believe these celebrity secrets!! Click to read!\"\n",
    "]\n",
    "for ex in examples:\n",
    "    print(ex, \"->\", predict(ex))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
